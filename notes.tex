\documentclass{notes}

\title{Real Analysis 1}
\subtitle{Dr. Freed}
\author{Neil Vyas}
\class{M 365C}
\date{Spring 2016}

\begin{document}

\maketitle
\tableofcontents

%TODO: definitions are
\begin{aside}{Theorems, Lemmas, etc.}
A Theorem is a proven statement in mathematics. \\
A Lemma is a smaller result on the way to proving a Theorem. \\
A Proposition is a smaller result that isn't deserving of the title ``Theorem.'' \\
A Corollary is a result that follows easily from another Theorem.
\end{aside}

\section{Number Systems}
\subsection{The Integers} % Lecture 1 - Jan 19th, 2016
Natural Numbers $\N$, $\Z^{>0} = \{1, 2, 3, \ldots\}$ equipped with the operation of addition.
Addition is a binary operation that is \emph{commutative} and \emph{associative.} Note that the 
identity element is zero, which is not in $\N$. \\

Non-negative integers $\Z^{\geq 0} = \{0 , 1, 2, \ldots\}$ is equipped with the same operation as 
$\N$, but also has the identity element 0. \\

The integers $\Z = \{\ldots, -1, 0, 1, \ldots\}$ are a countably infinite set that include inverse
elements for each integer. \\

\begin{aside}{Cardinalities}
Is $\Z$ bigger than $\Z^{\geq 0}$? How can we answer this question? We will approach it by examining 
some functions between the sets. We say that these sets are equal in ``size'' if we have a \emph{bijection}
between them; remember that a function is a bijection if it is both \emph{injective} and \emph{surjective}\\

The canonical inclusion is injective but not surjective, but we can still easily construct a bijection.
Consider that we really can find two equivalence classes of ``equal'' size in each set: in $\Z$, we have
the equivalence relation of sign, and in $\N$, we have the equivalence relation of even-ness. So send
the even numbers to half them, and the odd numbers to the negative numbers. \\
\end{aside}

Note that the integers equipped with addition and multiplication are just missing multiplicative inverses;
addition and multiplication are both associative, commutative, and have an identity element. We can
introduce a bigger number system to contain these multiplicative inverses, which we will call the rationals:
$$\Q = \{\frac{a}{b} \mid a, b \in \Z, b \neq 0\}.$$ 

How can we represent or describe $\Q$, starting from $\Z$? It looks like we can just do $\Z\times\Z^{\times}$, 
but this won't be correct since many points will be ``the same'' rational number. \\

%TODO
%put the defns of commutativity, associativity, identity, inverses here
%put the defns of rings and fields here

So $\Q$ is a field, and $\Z$ is a ring. \\

But $\Q$ still isn't big enough. What does this mean? For example, consider algebraic equations: relations
between elements of the field given by the operations of the field. We say $\Q$ isn't ``big enough'' 
because there are certain algebraic equations that do not admit rational solutions. \\

%TODO: format this
Let's consider the equation $x^2 = a$, for some $a\in\Q$. If $a = 4$, then we're golden, but what if 
$a = 2$? \\

\begin{theorem}
  There does not exist $x\in\Q \st x^2 = 2$. \\
\end{theorem}

We're going to prove the following lemmas on the way to proving this result:

\begin{lemma}
  $x\in\Q \Rightarrow \exists a,b\in\Z \text{ not even } \st x = \frac{a}{b}.$
\end{lemma}

\begin{proof}
  Since $x\in\Q$, we can choose $a,b\in\Z$ with $b > 0$ and minimal. Then if $a,b$ are both even, $b$ is
  not minimal, since we can halve both $a$ and $b$, but this is a contradiction. \\
\end{proof}

\begin{lemma}
  $a\in\Z, a^2 \text{ even} \Rightarrow a \text{ even}.$
\end{lemma}

\begin{proof}
The contrapositive is $a\text{ odd}\Rightarrow a^2\text{ odd},$ so let's try and prove that. Suppose
$a\in\Z$ and $a$ is odd. Then $\exists k\in\Z \st a = 2k + 1$. So 
\begin{align*}
  a^2 &= (2k + 1)^2 \\
  &= 4k^2 + 4k + 1 \\
  &= 2(2k^2 + 2k) + 1,
\end{align*}
and thus $a^2$ is odd. 
\end{proof}

So let's prove the theorem now. 

\begin{proof}
  Suppose not. That is, suppose TODO. Write $x = \frac{a}{b}, a,b\in\Z,
  b >0$, with not both $a,b$ even. Since $x^2 = 2$, we have $a^2 = 2b^2$, so by the lemma, since $a^2$ 
  is even, $a$ is even. But then $b^2$ must be even, since we can write $a = 2a^\prime$, and the result
  follows from there. So then both $a$ and $b$ are even, which is a contradiction. \\
\end{proof}



%lecture 2
\section{Relations}
\subsection{Orderings}
%defn
Let $S$ be a set. Then a \emph{relation} $R$ is a subset of $S\times S$. An \emph{order} is a relation
$R$ with the following properties: (for $x,y\in\R$)
\begin{enumerate}
\item $x,y\in S,$ exactly one of $x = y, x < y, x> y$ holds. Note that equality is in $S\times S$, not
  equality in the sense of our relation.
\item $x,y,z\in S, x<y, y<z \Rightarrow x<z$ \emph{(transitivity)}
\end{enumerate}

A set $S$ with an order is called an \emph{ordered set}. For example, $\Z\times\Z$ equipped with the
lexicographical ordering is an ordered set. However, we can see that $$(a,b) < (a', b') \Leftrightarrow
a + b < a' + b'$$ is not an ordering on $\Z\times\Z$ by considering the points $(3,2)$ and $(2,3)$. 

\subsection{Ordered Fields}
%defn ordered field
Let $F$ be a field equipped with an order $R$. Then $F$ is an \emph{ordered field} iff for $x,y,z\in F$, 
we have 
\begin{enumerate}
    \item $y < z \Rightarrow x+ y < x + z$
    \item $x > 0, y>0 \Rightarrow xy > 0$.
\end{enumerate}
Examples of ordered fields include $\Q$ with the usual ordering, Note that $\C$ is not an ordered field,
since $i^2 = -1$, which violates the second condition.

\begin{proposition}
  Let $F$ be an ordered field. Then 
  \begin{enumerate}
    \item $x > 0 \Rightarrow -x < 0$
    \item $x > 0, y<z \Rightarrow xy < xz$
    \item $x,y < 0 \Rightarrow xy > 0$
    \item $x < 0, y < z \Rightarrow xy > xz$
    \item $x\neq 0 \Rightarrow x^2 > 0$
    \item $1 > 0$
    \item $0 < x < y \Rightarrow 0 < \frac{1}{y} < \frac{1}{x}$
    \item $x,y < 0 \Rightarrow xy > 0$
  \end{enumerate}
\end{proposition}

\begin{proof}
  Get this from the book!
\end{proof}

\subsection{Complete Fields}
Note that we showed that $\Q$ is ``incomplete,'' since we had algebraic equations that had no solutions
in $\Q$ as well as sequences that did not converge in $\Q$. Now, we'll try to show this same statement 
using just the concept of order. \\

%defn
Let $S$ be an ordered set. 
\begin{enumerate}
  \item If $E\subset S, \beta\in S,$ and $\forall x\in E, x\leq\beta,$ then $\beta$ is an \emph{upper bound}
    of $E$. If such a $\beta$ exists, we say that $E$ is \emph{bounded above}.
  \item If $\alpha\in S$ is an upper bound of $E\subset S$ and $\forall \gamma < \alpha, \gamma$ is not
    an upper bound of $E$, we say that $\alpha$ is the \emph{least upper bound} of $E$.
  \item If $\forall E\subset S$ bounded above there exists a least upper bound, we say that $S$ has the
    \emph{least upper bound property}.
\end{enumerate}
The least upper bound is called the supremum, denoted $\sup$, while the greatest lower bound is called the
infimum, denoted $\inf$. \\

Consider $$E = \{x\in\Q \mid x^2 < 2 \} \subset S = \Q.$$ Note that this set is obviously bounded, above
and below. But $\sup E$ and $\inf E$ are not attained, since $\sqrt{2}\notin\Q$. Let's make this more
rigorous. \\

\begin{proposition}
  Write $\Q = E\cup E'$, where $E$ is as above and $E' = \{x\in\Q \mid x^2 > 2\}$. Then $E$ is bounded
  above and below but neither $\sup E$ nor $\inf E$ are attained, and $E'$ is unbounded.
\end{proposition}

\begin{proof}
  Suppose $\beta$ is an upper bound for $E$, and let $$\gamma = \frac{2\beta + 2}{\beta + 2}.$$ Then
  $$\gamma^2 - 2 = 2\frac{\beta^2 - 2}{(\beta + 2)^2}.$$ Since $\beta^2 > 2$ since it is an upper bound,
  we have that the whole expression is positive, and so we have that $\gamma^2 > 2 \Rightarrow \gamma$ is 
  an upper bound. Now 
  \begin{align*}
    \beta - \gamma &= \beta - 2\frac{\beta + 1}{\beta + 2} \\
                   &= \frac{\beta^2 + 2\beta - 2\beta - 2}{\beta + 2} \\
                   &= \frac{\beta^2 - 2}{\beta^2 + 2},
  \end{align*}
  so $\gamma < \beta$, which means that $\beta$ is not a least upper bound. Since $\beta$ is generic,
  there can be no least upper bound.
\end{proof}

\begin{theorem}
  There exists an ordered field $\R$ with the least upper bound property. Furthermore, $\Q\subset\R$
  as a subfield, i.e. there is an injective function $I:\Q\to\R$ which is compatible with the axoims
  of an ordered field, and $\R$ is unique.
\end{theorem}

%lecture 3
\begin{theorem}[Archimedean Property for $\R$]
  $x,y\in\R, x > 0 \Rightarrow \exists n\in\Z^{>0} \st nx > y$.
\end{theorem}
\begin{proof}
  Let $E = \{nx\mid n\in\Z^{>0}\}$. Suppose not. That is, suppose that $\forall n\in\Z^{>0}, y > nx$. 
  Thus, $y$ is an upper bound of $E$. By the least upper bound property of $\R$, let $\alpha = \sup E$.
  Then $\alpha - x < \alpha$ is not an upper bound for $E$. Thus, $\exists n\in\Z^{>0}\st nx > \alpha
  - x$. So we have that $$(n+1)x = nx + x > \alpha,$$ using the distributive property and the fact that
  $x > 0$, so we can add it to both sides and preserve the direction of the inequality. But since 
  $(n+1)\in\Z^{>0}$, we have that $\alpha$ is not an upper bound. 
\end{proof}
So we say that $\R$ is an \emph{Archimedean Field}.

\begin{theorem}[$\Q$ Dense in $\R$]
  $x,y\in\R, x < y \Rightarrow \exists r\in\Q \st x < r < y$.
\end{theorem}
This should be obvious, since we can just generate a rational from an irrational by truncation. So,
we just need to truncate far enough into the expansion to ensure that the truncation lies in the 
interval.Anyway, let's make this rigorous.
\begin{proof}
  Let $x,y$ be as above. Then we seek $m,n\in\Z, n\neq 0 \st x < \frac{m}{n} < y$. In order to get 
  more precision in the number of decimal places, we have to find $n$ large enough. So, we need that
  $$\frac{1}{n} < y-x,$$ i.e. choose $n$ sufficiently large that this condition holds. Note that we 
  can find such an $n$ because of the Archimedean property; now it only remains to find an appropriate
  $m$. We need $m$ to satisfy $nx < m$ and $m < ny$, both of which are separately satisfiable by the
  Archimedean property. \\

  Let's try and satisfy both separately and show that we can ``merge'' them into a single $m$. Let's 
  start with $m_2 < ny$. Since the Archimedean property lets us produce ``bigger'' numbers, let's 
  negate this inequality, yielding $-m_2 > -ny$. Now, by the Archimedean property, we have $m_1, 
  -m_2 > 0$ that satisfy both conditions and $m2 < 0 < m1$. Let $$M = \{m\in\Z\mid m_2 \leq m \geq m_1\}$$.
  Note that this set is non-empty and finite. However, we can't characterize the $m$ to pick from this
  set in terms of $m_1, m_2$ because we have no real constraints on $m_1$ and $m_2$, so they can be
  arbitrarily large or small. \\

  So we'll just specify an algorithm to find $m\in M$. Choose $m\in E \st m > nx \wedge (m - 1 \leq 
  nx \vee m = m_2)$.  Note that we can find this $m$ by starting at $\frac{m_2}{n}$ and incrementing
  by $\frac{1}{n}$. \\

  Does this work? We need to show that $x < \frac{m}{n} < y$; let's start with the $ < y$ direction.
  Given our conditions for $m$, we have that 
  \begin{align*}
    m - 1 &\leq nx \Rightarrow \\
    m &\leq nx + 1 \Rightarrow \\
    \frac{m}{n} &\leq x + \frac{1}{n} \\
                &< x + (y - x) \\
                &= y.
  \end{align*}
  We get the other direction for free from the conditions on $m$.
\end{proof}

\section{Spaces}
$\R^n$ is a \emph{vector space}, while $\E^n$ is a \emph{Euclidean space}. A Euclidean space is a 
vector space with the additional structure of an \emph{inner product}, $\ip{\cdot}{\cdot}:
\R^n\times\R^n\to\R$. The standard inner product is $\ip{x}{y} = \sum_{i=1}^n x_iy_i$. Any 
inner product must satisfy \emph{positivity}, \emph{reflexivity}, and \emph{symmetry}. Inner products
are \emph{linear}. \\

With this notion of inner product, we can go on to define some other useful properties of vectors,
like the length of a vector and the angle between two vectors. We can also derive a canonical \emph{norm}
as the length, with $||x||^2 = \left<x,x\right>$, which satisfies the \emph{triangle inequality}. \\

We also have that $$\cos\theta = \frac{\ip{\xi}{\eta}}{||\xi||\ ||\eta||}.$$

We say that two vectors $\xi, \eta$ are \emph{orthogonal} $\iff \ip{\xi}{\eta} = 0$. \\

So an inner product gives us a notion of \emph{geometry}: we have distance, angle, and length. \\

%lecture 3
There's some nonsense distinction between points and vectors, but I have never
really seen this before, and it seems like its only use is semantic disambiguation, so I'm tempted
to not include it with the tag ``I don't care.'' %\\

\begin{defn}
  A \emph{distance}, or a \emph{metric} $d: V \times V \to \R^{\geq 0}$ satisfies, for $p,q,r,\in V$
  and $V$ a set, usually an inner product space,
  \begin{enumerate}
    \item $d(p,q) \geq 0$, or \emph{positivity};
    \item $d(p,q) = 0 \iff p = q$, the \emph{identity of indiscernibles};
    \item $d(p,q) = d(q,p)$, or \emph{symmetry};
    \item $d(p,q) \leq d(p,r) + d(r,q)$, or the \emph{triangle inequality}.
  \end{enumerate}
\end{defn}

\begin{defn}
  A \emph{metric space} is a set $V$ together with a metric $d: V \times V \to \R^{\geq 0}$.
\end{defn}

\begin{proposition}
  A standard distance function for $p,q\in \E^n$ is $$d(p,q) = || p - q ||.$$
\end{proposition}
\begin{proof}
  Mostly work from definitions.
  \begin{enumerate}
      \item blah
      \item boooooring
      \item bleh
      \item We want to show that $$||\eta + \xi|| \leq |\eta|| + ||\eta||,$$ for $$r = p + \xi, q = 
        r + \eta.$$ So
        \begin{align*}
          ||\eta + \xi||^2 &= \ip{\eta + \xi}{\eta + \xi} \\
                           &= ||\xi||^2 + 2\ip{\xi}{\eta} + ||\eta||^2 \note{(bi-linearity of
                                $\ip{\cdot}{\cdot}$)} \\
                           &\leq ||\xi||^2 + 2||\xi||||\eta|| + ||\eta||^2 \note{(Cauchy-Schwartz)} \\
                           &= \left(||\eta|| + ||\xi||\right)^2,
        \end{align*}
        and since norms are positive, we have that $$||\eta + \xi|| \leq ||\eta|| + ||\xi||,$$ as
        desired.
  \end{enumerate}
\end{proof}

\begin{theorem}{Cauchy-Schwartz Inequality}
  For any vectors $\xi, \eta$, we have that $$|\ip{\xi}{\eta}| \leq ||\eta|| ||\xi||.$$
\end{theorem}
\begin{proof}
  \emph{(NB: The proof shown in class kinda sucks and doesn't generalize past 2 dimensions, so I'll
  omit it here. There's a nice slick proof that I'll steal from my linear algebra notes or
  something.)} \\

  Since norms are non-negative, we will again work by squaring both sides, which is an equaivalent
  statement. So we have to show $$|\ip{\xi}{\eta}|^2 \leq ||\eta||^2 ||\xi||^2.$$

  Hint for a nicer proof: consider the projection of $\eta$ onto the subspace spanned by $\xi +
  t\eta$.
\end{proof}

\begin{example}{Metric Spaces}
  \begin{enumerate}
      \item The empty set with the trivial metric.
      \item Any set $X$ equipped with 
        $$d_c(p,q) = \begin{cases} 0 &\text{ if } p=q \\ c\in\R^{>0} &\text{ if } p\neq q
        \end{cases},$$ the discrete map.
      \item The set of lines in $\E^n$ equipped with the smallest angle distance.  
      \item The set of lines in $\E^n$ equipped with the unit rectangle distance (define this).
  \end{enumerate}
\end{example}

%lecture 5
\begin{proposition}
  Let $X$ be the set of sequences $a_1, a_2, \ldots$, where $a_i \in \{0,1\}$. Then $X$ is uncountable.
\end{proposition}
\begin{proof}
  By Cantor's diagonal argument.
\end{proof}

\begin{defn}
  Let $(X,d)$ be a metric space, $E\subset X$ a subsapce, $p\in X$, $r\in\R^{>0}$. Then 
  \begin{enumerate}
    \item The \emph{open ball} of radius $r$ about $p$ is $$B_r(p) = \{ q\in X\mid d(p,q) < r.$$
    \item The \emph{deleted ball} of radius $r$ about $p$ is $$B_r'(p) = B_r(p)\setminus\{p\}.$$
    \item $E$ is \emph{open} if $$\forall p\in E, \exists \varepsilon > 0 \st B_\varepsilon(p) \subset E.$$
    \item $E$ is \emph{closed} iff $E^c$ is open.
    \item $E$ is \emph{bounded} if $\exists p\in X, R>0 \st E\subset B_R(p)$.
    \item $E$ is \emph{dense} iff $\forall p\in X, \varepsilon > 0, B_r(p)\cap E$ is non-empty.
    \item A point $p\in X$ is 
      \begin{itemize}
        \item an \emph{interior point} of $E$ iff $\exists \varepsilon > 0 \st B_\varepsilon(p) \subset E$.
        \item a \emph{limit point} of $E$ iff $\forall r > 0, B_r'(p)\cap E$ is non-empty.
        \item an \emph{isolated point} of $E$ if $\exists \varepsilon > 0 \st B_\varepsilon(p)\cap E = \{p\}$.
      \end{itemize}
  \end{enumerate}
\end{defn}

%lecture 6
\begin{theorem}
  A set $E$ is closed iff it contains all its limit points.
\end{theorem}

\begin{theorem}
  A finite set has no limit points.
\end{theorem}
\begin{proof}
  Let $E$ be a finite set. Then let $x$ be a limit point of $E$. Then for all $r > 0$, $B_r(x)\cap
  E$ is non-empty and hence $E$ contains an infinite number of points. So, we have the result.
\end{proof}

\begin{theorem}
  Let $(X,d)$ be a metric space. Then
  \begin{enumerate}
    \item An arbitrary union of open sets is open.
    \item An arbitrary intersection of closed sets is closed.
    \item A finite intersection of open sets is open.
    \item A finite union of closed sets is closed.  
  \end{enumerate}
\end{theorem}
\begin{proof}\leavevmode
  \begin{enumerate}
  \item Let $\{G_\alpha\}_{\alpha\in A}$ be a collection of open subsets of $X$. Suppose that $x\in\bigcup_{
  \alpha\in A}G_\alpha$. For some $a\in A$ we have that $x\in G_a$, and since $G_a$ is open $\exists r > 0$
  with $B_r(x)\subset G_a$. Hence, $B_r(x)\subset \bigcup_{\alpha\in A}G_\alpha$. Since $x$ was arbitrary,
  we have the claim.

  \item Let $\{G_f\}_{f\in F}$ be a collection of open subsets of $X$ indexed by finite $F$. Suppose that 
  $x\in\bigcap_{f\in F}G_f$. Then for each $f\in F$ we have that $x\in G_f$, and since $G_f$ is open
  $\exists r_f>0$ such that $B_{r_f}(x) \subset G_f$. \\

  Let $r = \min_{f\in F} r_f$. Then $B_r(x) \subset B_{r_f}(x) \subset G_f$ for all $f$. Thus, $B_r(x)
  \subset \bigcap_{f\in F}G_f$.

  \item Let $\{F_\alpha\}_{\alpha\in A}$ be a collection of closed sets. Then $\{F^c_\alpha\}_{\alpha
  \in A}$ is a collection of open sets. By (a), $\bigcup_{\alpha\in A}F^c_\alpha$ is open. Hence
  $$\left(\bigcup_{\alpha\in A}F^c_\alpha\right)^c = \bigcap_{\alpha\in A}\left(F_\alpha^c\right)^c
  = \bigcap_{\alpha\in A}F_\alpha. $$
  \end{enumerate} 
\end{proof}

\begin{defn}
  Let $\conj{E}$, the closure of a set $E\subset X$, be given by $$\conj{E} = E \cup E',$$ where $E'$
  is the set of limit points of $E\subset X$.
\end{defn}
\begin{theorem}\leavevmode
  \begin{enumerate}
    \item $\conj{E}$ is a closed set.
    \item $E$ is closed iff $E = \conj{E}$.
    \item $\conj{E}$ is the smallest closed set that contains $E$. That is, for a closed set $F$,
    $$E\subset F \Rightarrow \conj{E}\subset F.$$
  \end{enumerate}
\end{theorem}

\begin{defn}
  Let $(X,d)$ be a metric space, and $K\subset X$. Then 
  \begin{enumerate}
    \item A \emph{cover} of $K$ is a collection $\{G_\alpha\}_{\alpha\in A}$ of subsets of $X$ such 
      that $K \subset \bigcup_{\alpha\in A}G_\alpha$.
    \item Further, this cover is an \emph{open cover} iff each $G_\alpha$ is open.  
    \item $K$ is \emph{compact} iff every open cover of $K$ has a finite subcover. That is, if 
      $\{G_\alpha\}$ is an open cover of $K$, then there exists a finite subset $F\subset A$ such that
      $$K\subset \bigcup_{f\in F}G_f.$$
  \end{enumerate}
\end{defn}

\begin{theorem}
  $K\subset X$ is compact $\Rightarrow$ $K$ is closed.
\end{theorem}
\begin{proof}
  Suppose that $K$ is compact. Let $x$ be a point in $K^c$. For each $y\in K$, take $r_y = \frac{1}{3}
  d(x,y)$. Then $B_{r_y}(x)\cap B_{r_y}(y)$ is empty. Now, the collection of $B_{r_y}(y)$ forms an open
  cover of $K$, since each point in $k$ is contained in at least one such ball (it is the center of 
  one). Since $K$ is compact, we take a finite subcover, indexed by $F$ a finite set. \\

  Now, take $$r = \min_{y\in F}r_y.$$ Then $B_r(x) \cap K$ is empty.
\end{proof}

\begin{theorem}
  $K\subset X$ compact, $C\subset K$, $C$ closed in $X$ $\Rightarrow$ $C$ is compact.
\end{theorem}
\begin{corollary}
  If $(X,d)$ is a compact metric space, $C\subset X$ is closed $\Rightarrow C$ is compact.
\end{corollary}
\begin{theorem}
  $K\subset X$ is compact iff every infinite subset $E\subset K$ has a limit point in $K$.
\end{theorem}
\begin{proof}

\end{proof}

\begin{theorem}[Heine-Borel]
  A set $K\subset\E^k$ is compact iff it is closed and bounded.
\end{theorem}

\begin{lemma}
  Let $\{I_n\}_{n=1}^\infty$ be a collection of closed intervals in $\R$ with $I_n = [a_n, b_n],
  a_n < b_n$. Assume $I_{n+1} \subset I_n$ for all $n$. Then $$\bigcap_{n=1}^\infty I_n \neq \emptyset.$$
\end{lemma}
\begin{proof}
  The set $\{a_n\}_{n=1}^\infty$ is bounded above by $b_1$ and is nonempty, so there exists
  $$x = \sup_n \{a_n\}.$$ Then $\forall n, a_n\leq x$, and for $m>n$ we have that
  $a_n \leq a_m < b_m \leq b_n,$ so $b_m$ is an upper bound of $\{a_n\}_{n=1}^\infty$, and so 
  $x\leq b_n$. So $$\forall n, x\in [a_n, b_n] = I_n.$$
\end{proof}

\begin{lemma}
  $\{I_n\}$ is a sequence of nested rectangles $\in\E^2$, that is, $$I_n = [a_n^{(1)}, b_n^{(1)}]
  \times [a_n^{(2)}, b_n^{(2)}].$$ Then $$\bigcap_{n=1}^\infty I_n = \emptyset.$$
\end{lemma}
\begin{proof}
  By projecting each $I_n$ onto each axis and then applying the previous lemma.
\end{proof}

\begin{proposition}
  A rectangle, or product of two closed intervals, in $\E^2$ is compact.
\end{proposition}
\begin{proof}
  Suppose that $\{G_\alpha\}$ is an open cover. Suppose not; that is, suppose that there is no finite
  subcover. We proceed by an iterative process. Divide the rectangle into finitely many pieces; then
  if there is no finite subcover, one (WLOG) of these pieces must not have a finite subcover. Repeat
  this iteratively, giving a sequence of nested rectangles that have nonempty intersection. Then by the 
  previous lemma, the arbitrary intersection is nonempty; say that $p$ is contained in this intersection.
  Then, since $p$ is in the rectangle, there exists (WLOG) some $\alpha_0\in A$ such that $p\in G_{\alpha_0}$.
  But this is a contradiction, since for some rectangle in this iterative process, it is contained 
  in $G_{\alpha_0}$ as well, because the rectangles are nested, which means it has a finite subcover.
  So we have arrived at a contradiction, and thus this rectangle is compact. \\

  For every WLOG, where we say ``one,'' it can be replaced by ``finitely many,'' which doesn't alter
  the mechanism of the proof.
\end{proof}


\begin{proof}%theorem
  $(\Leftarrow)$
  $(\Rightarrow)$  
\end{proof}


\begin{theorem}
  For $\{K_\alpha\}_{\alpha\in A}$ a collection of compact subsets of $X$, assume $\bigcap K_\alpha$
  is not empty for all finite intersections. Then $$\bigcap_{\alpha\in A} K_\alpha \neq\emptyset.$$
\end{theorem}
\begin{proof}
  Suppose not; that is, suppose that $\{K_\alpha\}_{\alpha\in A}$ is as above, but 
  $\bigcap_{\alpha\in A} K_\alpha = \emptyset.$ Fix $\alpha_0\in A$. Then we have that $\{K_\alpha^c\}
  _{\alpha\in A \neq \alpha_0}$ is an open cover of $K_{\alpha_0}$. This is because for each $\alpha$,
  $K_\alpha^c$ is open (since $K_\alpha$ is closed), so this collection of sets is open, and it is a cover
  of $K_{\alpha_0}$ since if $p\in K_{\alpha_0}$ is not in $\bigcup\{K_\alpha^c\}$, then $p\in K_\alpha,
  \alpha\neq{\alpha_0}$, and so $p\in\bigcap_{\alpha\in A}K_\alpha$, which contradicts our assumption 
  that this intersection is empty. Since we have an open cover and $K_{\alpha_0}$ is compact, we have a 
  finite subcover $\{\alpha_1, \ldots, \alpha_n\}\subset A$ such that $K_{\alpha_0} \subset 
  \left(K^c_{\alpha_1} \cup \ldots \cup K^c_{\alpha_n}\right)$. Thus, we have that $$K_{\alpha_1}
  \cup \ldots \cup K_{\alpha_n} \subset K^c_{\alpha_0},$$ which means that $K_{\alpha_1} \cup \ldots
  \cup K_{\alpha_n} \cup K_{\alpha_0} = \emptyset,$ which contradicts our initial assumption that
  finite subcollections indexed by a $A$ have non-trivial intersection. So, we have the claim.
\end{proof}

\begin{corollary}
  If $\{K_n\}^\infty_{n=1}$ is a sequence of nonempty compact subsets of $X$ and if $K_{n+1}\subset 
  K_n$ for all $n$, then $$\bigcap_{n=1}^\infty K_n \neq \emptyset.$$
\end{corollary}
\begin{proof}
  By the theorem, all finite intersections are nonempty, so the conclusion holds.
\end{proof}

\begin{corollary}
  $K\subset X$ compact, $E\subset K$ is an infinite subset $\Rightarrow E$ has a limit point in $K$.
\end{corollary}
\begin{proof}
  Suppose that there is no point of $K$ that is a limit point of $E$. Then $\forall p\in K, \exists
  r_p >0 \st B_{r_p}(p) \cap E$ is finite. Then $\{B_{r_p}(p)\}_{p\in K}$ is an open cover of $K$. 
  Since $K$ is compact, there exists a finite subcover $\{B_{r_p}(p_i)\}_{i=1}^n$. Then $E$ is also
  covered by this finite subcover, and since the intersection of $E$ with every ball is finite, we 
  have that $E$ is finite. This contradicts our assumption that $E$ is infinite, and so we have the claim.
\end{proof}

%lecture 8
%TODO: refactor the other theorems to be parts of this proof.
\begin{theorem}
  Let $K\subset\E^n$. Then the following are equivalent:
  \begin{enumerate}
    \item $K$ is closed and bounded;
    \item $K$ is compact;
    \item Every infinite $E\subset K$ has a limit point in $K$.  
  \end{enumerate}
\end{theorem}
\begin{proof}
  We proved each of the three implications separately.
\end{proof}

\begin{defn}
  Let $(X,d)$ be a metric space. Then 
  \begin{enumerate}
    \item Subsets $A,B\subset X$ are \emph{separated} iff $A \cap \conj{B}$ and $\conj{A} \cap B$ are
      both empty.
    \item $X$ is \emph{connected} iff it is not the union of two separated subsets.  
  \end{enumerate}
\end{defn}
Intuitively, separated sets don't abut each other, i.e. they share no limit points that one contains.

\begin{theorem}
  A subset $E\subset\R$ is connected iff $\forall x,y\in E$ with $x < y$ and $z\in\R$ such that 
  $x < z < y$, then $z\in E$. That is, $x,y\in E, x < y \Rightarrow (x,y)\subset E$.
\end{theorem}

\section{Sequences}
%material from lecture 2
We say a sequence is a function from $\N\to\Q$. Consider $$a_n = 1 + \frac{1}{1!} + \frac{1}{2!} + \ldots$$ 
Note that this is a function of the number of terms (in $\N$), and each term is in $\Q$. But this sequence
is ``converging'' to $e$, which is not in $\Q$. Additionally, consider the following sequence:
$$b_n = (1 + \frac{1}{n})^n.$$
Note that both these sequences converge to $e$, but the second converges much faster.\\

We will define the real numbers $\R$ in a similar manner, with these limit processes. \\

\begin{defn}
  Let $(X,d)$ be a metric space. A \emph{sequence} in $X$ is a function $p: \N\to X$, often denoted
  as $p = \{p_n\}$, where $p_n = p(n)$.
\end{defn}

\begin{defn}
  Let $\{p_n\} \subset X$ be a sequence. Then if $n_1< n_2< \ldots$ are positive integers, then 
  $\{p_{n_i}\}$ is a \emph{subsequence} of $\{p_n\}$, where $i\to n_i$ is an increasing injection.
\end{defn}

\begin{defn}
  Let $\{p_n\}\subset X$ be a sequence. Then $\{p_n\}$ \emph{converges} to $p\in X$ iff
  $$\forall \varepsilon > 0, \exists N\in\N \st \forall n\geq N, d(p_n, p) < \varepsilon.$$
\end{defn}

\begin{defn}
  Let $\{p_n\}\subset X$ be a sequence. Then $\{p_n\}$ is a \emph{Cauchy sequence} iff 
  $$\forall \varepsilon > 0, \exists N\in\N \st \forall n,m \geq N, d(p_n, p_m) < \varepsilon.$$
\end{defn}

\begin{theorem}
  Let $(X,d)$ be a metric space, $\{p_n\} \subset X$. Then
  \begin{enumerate}
    \item If $p, p' \in X$ with $p_n \to p$ and $p_n \to p'$, then $p = p'$.
    \item If $\{p_n\}$ converges, then it is bounded.
    \item If $\{p_n\}$ converges, then it is Cauchy.
    \item If $\{p_n\}$ is Cauchy and $X$ is compact, then $\{p_n\}$ converges.
    \item If $\{p_n\}$ is Cauchy and $X = \E^k$, then $\{p_n\}$ converges.
    \item If $\{p_n\}$ is Cauchy and a subsequence converges, then $\{p_n\}$ converges.
  \end{enumerate}
\end{theorem}
%lecture 9
\begin{proof}
  I'll just provide sketches, since these are all easy and follow from the same core idea.
  \begin{enumerate}
    %uniqueness of limit
  \item Suppose not, then $d(p, p') \neq 0$; since $\{p_n\}$ converges to both, we can get arbitrarily
  close to both $p, p'$, which will allow us to violate the triangle inequality. \\

  That is, suppose $d(p,p') = \varepsilon$. By convergence, we have $$\exists p_n \st d(p, p_n) < 
  \frac{1}{3} \varepsilon,\ d(p_n, p') < \frac{1}{3}\varepsilon.$$ But then
  \begin{align*}
    d(p, p') &\leq d(p, p_n) + d(p_n, p') \\\
             &< \frac{2}{3}\varepsilon,
  \end{align*}
  a contradiction.
  %converges -> bounded
  \item We need every element of the sequence to fit inside a ball. But from convergence, we know that
    the sequence is bounded by $B_\varepsilon(p)$ for some finite $N$. Then there are at most $N$ points
    that are outside of this ball, so take the max distance of those points from $p$ to be the radius
    of a ball centered at $p$, which will then contain $\{p_n\}$.
  %converges -> Cauchy
  \item Just control the distance between two points by using the triangle inequality with $p$.
  \end{enumerate} 
\end{proof}

\begin{theorem}
  $p_n\to p$ iff $\forall \varepsilon > 0 , B_\varepsilon(p)$ contains all but finitely many elements
  of $\{p_n\}$.
\end{theorem}
\begin{proof}
  follows from the definition of convergence.
\end{proof}

\begin{theorem}
  If $X$ is a compact metric space, $\{p_n\} \subset X$, then there is a subsequence $\{p_{n_i}\}$
  which converges in $X$.
\end{theorem}
\begin{proof}
  If $p$ has finite range, then pick a constant subsequence, which obviously converges. The constant 
  subsequence exists by the pigeonhole principle. If $p$ has infinite range $\Img(p) = E \subset X$,
  then $E$ is an infinite subset of a compact metric space, and so by (Thm 3.8), $E$ has a limit point
  $p\in X$. Then choose $n_1 \st d(p_{n_1}, p) < 1,$ and inductively choose $$n_{i+1} \st n_{i+1} > n_i,
  \ d(p_{n_{i + 1}}, p) < \frac{1}{i + 1}.$$ So then $\{p_{n_i}\}$ converges to $p$, as desired.
\end{proof}

\begin{corollary}
If $X$ is compact, $\{p_n\}$ Cauchy,  then $\{p_n\}$ converges.
\end{corollary}
\begin{proof}
 By the theorem, there exists a convergent subsequence. By a homework problem, since $\{p_n\}$ is 
 Cauchy and there is a convergent subsequence, then $\{p_n\}$ converges.
\end{proof}

\begin{corollary}
  If $\{p_n\} \subset \E^k$ is Cauchy, then $\{p_n\}$ converges.
\end{corollary}
\begin{proof}
  I will provide the sketch of the proof. First, note that $\{p_n\}$ is bounded, by a prior result.
  Then choose $R > 0, p\in\E^k  \st \{p_n\} \subset B_R(p) \subset \conj{B_R(p)}.$ Since $\conj{B_R(p)}$
  is closed and bounded, it is compact. Hence, $\{p_n\}$ converges.
\end{proof}

\begin{defn}
  A metric space is complete iff every Cauchy sequence converges.
\end{defn}

%lecture 10
\begin{lemma}
Let $\{a_n\}, \{b_n\} \subset \R, a,b \in \R,$ and suppose that 
\begin{itemize}
  \item $a_n \to a$;
  \item $b_n \to b$;
  \item $\forall n, a_n \leq b_n$.
\end{itemize}
Then $a \leq b$. (Note that we cannot use this method to conclude $a < b$, since in the limit $a_n$ and
$b_n$ may coincide even if $a_n < b_n$.)
\end{lemma}
\begin{proof}
  Suppose not, that is, suppose that $a > b$. Let $\delta = a-b$. Then by the definition of convergence,
  $$\exists N\in\N \st \forall n \geq N, |a-a_n| < \frac{\delta}{2}, |b - b_n| < \frac{\delta}{2}.$$
  Then 
  \begin{align*}
    b_n &< b + \frac{\delta}{2} \\
        &= \frac{a + b}{2} \\
        &= a - \frac{\delta}{2} \\
        &< a_n,
  \end{align*}
  a contradiction. So we have that $a < b$.
\end{proof}

\begin{lemma}
  If $\{a_n\} \subset \R$ and $|a_n| \to 0$, then $a_n \to 0$.
\end{lemma}
\begin{proof}
  Given $\varepsilon > 0$, we have that $$\exists N\in\N \st \forall n \geq N, \varepsilon > ||a_n| - 
  0| = |a_n - 0|,$$ and so by the definition of convergence $a_n \to 0$. Note that the converse is also
  true, by the same proof.
\end{proof}

\begin{defn}
  A sequence is \emph{monotone increasing} if\/ $\forall n, s_{n+1} \geq s_n$, and similarly for 
  \emph{monotone decreasing}.
\end{defn}
\begin{lemma}
  $\{s_n\}$ is monotone increasing iff $\{-s_n\}$ is monotone decreasing.
\end{lemma}

\begin{theorem}
  Let $\{s_n\}$ be monotone. Then $\{s_n\}$ converges iff it is bounded.
\end{theorem}
\begin{proof}
  $(\Rightarrow)$ This follows from the general theorem for convergent sequences in a metric space. \\

  $(\Leftarrow)$ Since $\{s_n\}$ is bounded above, there exists a least upper bound $s = \sup \{s_n\}$.
  Now, we must show that $s_n \to s$, which we will do by the definition. Given $\varepsilon > 0$ and 
  since $s$ is the least upper bound, we have that $$\exists N\in\N \st s_N > s - \varepsilon,$$ since
  $s - \varepsilon$ is not an upper bound. Then by induction we have that $$\forall n \geq N, s \geq 
  s_n \geq s_N > s-\varepsilon,$$ and so $|s - s_n| < \varepsilon$, as desired. \\

  Now we must prove the induction we referenced earlier. We induct on the statement $$\forall N\in\N,
  k = 0, 1, 2, \ldots, s_{N + k} \geq s_N,$$ which arises from monotonicity.
  
\end{proof}

\begin{theorem}\leavevmode
  Some useful limits.
  \begin{enumerate}
    \item If $p > 0$, then $\lim_{n\to\infty} \frac{1}{n^p} = 0$.
    \item If $|x| > 1$, then $\lim_{n\to\infty} x^n = 0$.
    \item $\lim_{n\to\infty} n^{(1/n)} = 1$.  
  \end{enumerate}
\end{theorem}
\begin{proof}\leavevmode
  \begin{enumerate}
    \item 
      \begin{align*}
        &\frac{1}{n^p} < \varepsilon \\
        &\Rightarrow 1 < \varepsilon n^p \\
        &\Rightarrow \frac{1}{\varepsilon} < n^p \\
        &\Rightarrow \left( \frac{1}{\varepsilon} \right)^{(1/p)} < n.
      \end{align*}

    \item It's enough to show the result for $0 \leq x < 1$, and the $x = 0$ case is trivial. So assume
      $0 < x < 1$. Then let $x = \frac{1}{1 + y}$ for some $y > 0$. Then $y = \frac{1 - x}{x}$. Finally,
      by the binomial formula we have that
      \begin{align*}
        x^n &= \frac{1}{(1 + y)^n} \\
            &= \frac{1}{1 + ny + (\frac{n}{2}) y^2 + \ldots} \\
            &\leq \frac{1}{1 + ny},
      \end{align*}
      and 
      \begin{align*}
        &\frac{1}{1 + ny} < \varepsilon \\
        &\Rightarrow \frac{1}{\varepsilon} < 1 + ny \\
        &\Rightarrow \frac{\frac{1}{\varepsilon} - 1}{y} < n,
      \end{align*}
      so $\frac{1}{1 + ny} \to 0$.

    \item Set $x_n = n^{1/n} - 1$; we want to show that $x_n \to 0$. Note that $x_n > 0$.
    
  \end{enumerate}
\end{proof}

%lecture 11
\begin{defn}[Convergence of Sequences]
  Let $\{a_n\} \subset \R$. Then the \emph{partial sums} of the series $\sum a_n$ are the elements of
  the sequence $$s_n = \sum_{k=1}^n a_n = a_1 + a_2 + \dots + a_n.$$ We say that $\sum a_n$ converges
  iff $\{s_n\}$ converges, and if it has limit $s$, then we write $$\sum_{k=1}^\infty a_k = s.$$
\end{defn}

\begin{proposition}
  $$\sum_{n=1}^\infty \frac{1}{n^2} = \frac{\pi^2}{6}.$$ 
\end{proposition}
\begin{proof}
  Obvious.
\end{proof}

\begin{proposition}\leavevmode
  \begin{enumerate}
    \item $\sum a_n$ converges iff $$\forall \varepsilon > 0, \exists N\in\N \st \forall n,m \geq N,
      \left| \sum_{k=n}^m a_k \right| < \varepsilon.$$
    \item $\sum a_n$ converges $\Rightarrow a_n \to 0$.
  \end{enumerate}
\end{proposition}
\begin{proof}\leavevmode
  \begin{enumerate}
    \item Observe that $$\sum_{k=n}^m a_k = s_m - s_{n-1},$$ and so the statement is equaivalent to the 
      statement that $s_n$ is Cauchy. Since $\R$ is complete, and $\{s_n\}$ is Cauchy, we have that 
      $\{s_n\}$ converges, and so $\sum a_n$ converges.
    \item Take $m = n$ in (1), and suppose that $\sum a_n$ converges. Thus, for any $\varepsilon > 0$, 
      we have found $n \geq N \Rightarrow |a_n| < \varepsilon$. Therefore, $a_n \to 0$. Note that the 
      converse is not true; take $\sum 1/n$.
  \end{enumerate}
\end{proof}

\begin{proposition}
  $\sum 1/n$ diverges.
\end{proposition}
\begin{proof}
  \begin{align*}
    \sum_n \frac{1}{n} &= 1 + \frac{1}{2} + \left( \frac{1}{3} + \frac{1}{4} \right) + \left( \frac{1}{5} + \frac{1}{6} + \frac{1}{7} + \frac{1}{8} \right) + \dots \\
                       &\geq 1 + \frac{1}{2} + \left( \frac{1}{4} + \frac{1}{4} \right) + \left( \frac{1}{8} + \frac{1}{8} + \frac{1}{8} + \frac{1}{8} \right) + \dots \\
                       &= 1 + \frac{1}{2} + \frac{1}{2} + \dots 
  \end{align*}
\end{proof}

\begin{theorem}[Cauchy Twin Series]
  Suppose that $\{a_n\}$ is monotonically decreasing and bounded below by 0. Then $$\sum_{n=1}^\infty a_n
  \text{ converges } \iff \sum_{k=1}^\infty 2^k a_{2^k} \text{ converges.}$$
\end{theorem}
\begin{proof}
  Let $s_n$ be the partial sums of $\{a_n\}$ and $$t_k = a_1 + 2a_2 + 4a_4 + \dots + 2^ka_{2^k}.$$
  If $n < 2^k,$ then 
  \begin{align*}
    s_n &\leq a_1 + (a_2 + a_3) + (a_4 + a_5 + a_6 + a_7) + \dots + a_{2^{k - 1}} \\
        &\leq a_1 + (a_2 + a_2) + (a_4 + a_4 + a_4 + a_4) + \dots + a_{2^{k - 1}} \\
        &= a_1 + 2a_2 + 4a_4 + \dots + 2^{k-1}a_{2^{k-1}} \\
        &= t_{k-1}.
  \end{align*}
  If $n \geq 2^k,$ we do similarly
  \begin{align*}
    s_n &\geq a_1 + a_2 + (a_3 + a_4) + (a_5 + a_6 + a_7 + \dots + a_{2^k} \\
        &\geq a_1 + a_2 + (a_4 + a_4) + (a_8 + a_8 + a_8 + \dots + a_{2^k} \\
        &\geq \frac{1}{2}a_1 + 2a_4 + 4a_8 + \dots + 2^{k-1}a_{2^k} \\
        &= \frac{1}{2}\left( a_1 + 2a_2 + 4a_4 + \dots + 2^ka_{2^k} \right) \\
        &= \frac{1}{2}t_k.
  \end{align*}
  So either both are bounded or both are unbounded, and since both are monotonically increasing, either
  both converge or both diverge.
\end{proof}

\begin{corollary}[$p$-Series]
  $\sum 1/(n^p)$ converges iff $p > 1$.
\end{corollary}
\begin{proof}
  If $p \leq 0$ then $1/(n^p) = n^{-p} \not\to 0,$ and so the series diverges. For $p > 0$, we'll 
  apply the twin-series theorem. So let $a_n = 1/(n^p)$ and so $$2^ka_{2^k} = 2^k \frac{1}{(2^k)^p} = 
  \frac{2^k}{2^{kp}} = \left( \frac{1}{2^{p-1}} \right)^k.$$ This is now a geometric series, which converges
  iff $1/(2^{p-1}) < 1 \iff p > 1,$ which we'll prove below.
\end{proof}

\begin{defn}[Geometric Series]
  For $x\in\R$ the series $\sum x^n$ is called a \emph{geometric series} with ratio $x$.
\end{defn}

\begin{theorem}[Convergence of Geometric Series]
  $\sum x^n$ converges iff $|x| < 1$, and further $$\sum_{n=0}^\infty x^n = \frac{1}{1 - x}.$$
\end{theorem}
\begin{proof}
  If $|x| \geq 1$, then $x_n \not\to 0,$ so $\sum x^n$ diverges. So suppose that $|x| < 1.$ So then
  $s_{n} - xs_n = 1 - x^{n+1}$, and so $$s_n = \frac{1 - x^{n+1}}{1 - x}.$$ Since $|x| < 1$, we have
  that $x^{n+1} \to 0$, and so $$\lim_{n\to\infty} s_n = \frac{1}{1-x}.$$
\end{proof}

\begin{aside}{Complex Numbers}
Note that since $\C \sim \R^2$, we have that complex power series $\sum z^n$ have not an interval of 
convergence but a \emph{radius} of convergence. There's also a lot of other nice stuff that goes along 
with this, but we'll leave that for a course in Complex Analysis.
\end{aside}

\begin{lemma}
  $\sum 1/(n!)$ converges.
\end{lemma}
\begin{proof}
  Since $\{s_n\}$ is monotonically increasing, it suffices to show that it is bounded above. Since 
  $n! \geq n^2$ for $n > 2$, we can bound the partial sums above by 3.
\end{proof}

\begin{defn}[$e$]
  $$e = \sum_{n=0}^\infty \frac{1}{n!}.$$
\end{defn}

\begin{theorem}
  $e$ is irrational.
\end{theorem}
\begin{proof}
  Suppose not; that is, suppose that $e = p/q$ for $p,q \in \Z_{\geq 0}, q\neq 0$. Then 
  \begin{align*}
    q! (e - s_q) &= q! \left( \frac{1}{(q + 1)!} + \frac{1}{(q + 2)!} + \dots \right) \\
                 &= \frac{1}{q + 1} + \frac{1}{(q+1)(q+2)} + \frac{1}{(q+1)(q+2)(q+3)} + \dots \\
                 &\leq \frac{1}{q + 1} + \left( \frac{1}{q + 1} \right)^2 + \left( \frac{1}{q + 1} \right)^3 \\
                 &= \frac{1}{1 - \frac{1}{q + 1}} - 1 \\
                 &= \frac{1}{q}.
  \end{align*}
  But
  \begin{align*}
    q!(e - s_q) &= q! \left( \frac{p}{q} + \left( 1 + 1 + \frac{1}{2!} + \frac{1}{3!} + \dots \right) \right) \\
                &= (q-1)!p - \left( q! + q! + \frac{q!}{2!} + \frac{q!}{3!} + \frac{q!}{q!} \right),
  \end{align*}
  and this entire expression is an integer. But we had that $$q!(e - s_q) \in \left( 0, \frac{1}{q} \right),$$
  and there are no integers in this interval. So we have a contradiction.
\end{proof}

%lecture 12 (13?)
\begin{theorem}[Root Test]
  For a series $\sum a_n$, let $$\alpha = \limsup_{n\to\infty} \sqrt[n]{|a_n|}.$$ Then 
  \begin{enumerate}
    \item $\alpha > 1 \Rightarrow \sum a_n$ diverges; 
    \item $\alpha < 1 \Rightarrow \sum a_n$ converges;
    \item $\alpha = 1 \Rightarrow \sum a_n$ may converge or may diverge.
  \end{enumerate}
\end{theorem}

\begin{lemma}
  $$x > \limsup_{n\to\infty} a_n \Rightarrow \exists N\in\N \st \forall n \geq N, a_n < x.$$
\end{lemma}
\begin{proof}
  Just do it!
\end{proof}

\begin{proof}\leavevmode
 \begin{enumerate}
   \item Choose a subsequence $\sqrt[n_i]{|a_{n_i}|} \to \alpha$; we know this exists by definition of 
     $\limsup.$ Then since $\alpha > 1$, by the definition of $\limsup$, $$\exists I\in\N \st \forall 
     i \geq I, \sqrt[n_i]{|a_{n_i}|} > 1 \Rightarrow |a_{n_i}| > 1 \Rightarrow a_n \not\to 0.$$

   \item Choose $\beta \st \alpha < \beta < 1$. Then 
       $$\exists N\in\N \st \forall n \geq N, \sqrt[n]{|a_n|} < \beta \Rightarrow |a_n| < \beta^n.$$
       So 
       \begin{align*}
         \sum_{n=1}^\infty |a_n| &= \sum_{n=1}^N |a_n| + \sum_{n=N+1}^\infty |a_n| \\
                                 &\leq \sum_{n=1}^N |a_n| + \sum_{n=N+1}^\infty \beta^n \\
                                 &\leq \sum_{n=1}^N |a_n| + \frac{1}{1-\beta},
       \end{align*}
       which is finite.

   \item Consider $\sum (1/n)$; note that since $\sqrt[n]{n} \to 1$, we have that $\alpha = 1$, and we 
   know that this series diverges. Now consider $\sum (1/n^2)$; we have that $n^{-(1/n) \cdot 2} \to 1$,
   so $\alpha = 1$, but this series converges.
 \end{enumerate} 
\end{proof}

\begin{theorem}[Ratio Test]
  blah
\end{theorem}

\begin{defn}[Power Series]
  Let $\{a_n\} \subset \R$. The \emph{power series with coefficients} $\{a_n\}$ is $$\sum_{n=1}^\infty
  a_nx^n.$$
\end{defn}

\begin{theorem}
  Let $$\alpha = \limsup_{n\to\infty}\sqrt[n]{|a_n|}, \qquad R = \frac{1}{\alpha}.$$ Then 
  \begin{enumerate}
    \item $|x| > R \Rightarrow \sum a_nx^n$ diverges; 
    \item $|x| < R \Rightarrow \sum a_nx^n$ converges;
    \item $|x| = R \Rightarrow \sum a_nx^n$ may converge or may diverge.
  \end{enumerate} 
\end{theorem}
\begin{proof}
  apply root test
\end{proof}

\section{Continuity and Limits}
\subsection{Continuous Functions}
\begin{theorem}[Compact sets to compact sets]
  If $X,Y$ are metric spaces with $X$ compact, and $f: X \to Y$ is continuous, then $f(X) \subset Y$
  is compact.
\end{theorem}
\begin{proof}
  Suppose $\{V_\alpha\}_{\alpha\in A}$ is an open cover of $f(X)$. Now, we need to produce a finite
  subcover. Since $f$ is continuous, each $f^{-1}(V_\alpha) \subset X$ is open, and also that
  $\{f^{-1}(V_\alpha)\}_{\alpha\in A}$ is an open cover of $X$. Since $X$ is a compact metric space,
  take a finite subcover indexed by $A' \subset A$. Applying $f$ to each set in this finite
  subcover, we find that we have a finite subcover of $f(X)$. That is, 
  \begin{align*}
    X &= \bigcup_{\alpha\in A'} f^{-1}(V_\alpha); \\ 
    f(X) &= \bigcup_{\alpha\in A'} f(f^{-1}(V_\alpha)) \\ 
         &\subset \bigcup_{\alpha\in A'} V_\alpha.
  \end{align*}
\end{proof}

Let's give another proof using a different characterization of continuous functions.
\begin{proof}
  Suppose $\{q_n\} \subset f(X)$ is a sequence. Choose $p_n \in X \st f(p_n) = q_n$ (pick a point in
  the fibre); now we have a sequence in $X$. Since $X$ is compact, there exists a convergent
  subsequence $p_{n_i} \to p$ for some $p\in X$. By continuity of $f$, we have that 
  $$f(p_{n_i}) = q_{n_i} \to f(p) \in Y.$$ So we have that $f(X)$ is compact.
\end{proof}

\begin{corollary}
  If $X$ is a compact metric space, $f: X \to \E^k$ is continuous, then $f(X)$ is closed and
  bounded.
\end{corollary}
\begin{proof}
  By the theorem, we have that $f(X)$ is compact, and by Heine-Borel we have that it must be closed
  and bounded.
\end{proof}

\begin{corollary}
  If $X$ is a compact metric space, $f:X \to \R$ is continuous, then $f$ achieves its supremum and
  infimum.
\end{corollary}
\begin{proof}
  $f(X) \subset \R$ is closed and bounded by the corollary. Then, since a closed subset of $\R$
  contains its supremum and infimum, we have the result.
\end{proof}

\begin{theorem}
  Suppose that $X,Y$ are metric spaces, with $X$ compact, and $f:X\to Y$ is a continuous bijection.
  Then $f^{-1}: Y\to X$ is continuous. Note that this fails if $X$ is not compact.
\end{theorem}
\begin{proof}
  We will show that $$C \subset X \text{ closed} \Rightarrow f(C) \subset Y \text{ closed.}$$
  Suppose $C\subset X$ is closed. Then $C$ is compact, since it is a closed subset of a compact
  metric space. So $f(C)$ is also compact, by the previous theorem. Thus, $f(C)$ is also closed.  
\end{proof}

\begin{defn}[Uniformity]
  $f: X \to Y$ is \emph{uniformly continuous} if we can use the same tolerance for all points in the domain;
  that is, $$\forall \varepsilon > 0, \exists \delta \st \forall p\in X, P(\varepsilon, \delta,
  p),$$ where $P$ is the predicate from continuity.
\end{defn}

\begin{theorem}[Uniformity from Compact Domain]
  Suppose $X,Y$ are metric spaces with $X$ compact, and $f: X\to Y$ continuous, then $f$ is
  uniformly continuous.
\end{theorem}
\begin{proof} %proved in lecture 15
 Suppose not; that is, suppose that $X$ is compact but $f$ is not uniformly continuous. Then
 $$\exists \varepsilon > 0 \st \forall \delta > 0, \exists p,q\in X \st d(p,q) < \delta \text{ and }
 d(f(p), f(q)) \geq \varepsilon.$$ Fix $\varepsilon > 0$. So we can find $\{p_n\}, \{q_n\} \subset
 X$ such that $d(p_n, q_n) < 1/n = \delta$ and $d(f(p_n), f(q_n)) \geq \varepsilon$. Since $X$ is
 compact, we have $\{p_{n_i}\} \to p$ for some $p\in X$. Thus, $\{q_{n_i}\} \to p$ for a convergent
 subsequence $\{q_{n_i}\}$ (obvious via triangle inequality). So since $f$ is continuous, we must
 have that $f(p_{n_i}) \to f(p)$ and $f(q_{n_i}) \to f(p)$, which is then a contradiction, since
 this contradicts our assumption that $f$ is not uniformly continuous.
\end{proof}

When we try to interchange infinite processes - limits, summations, and integrals - we will need
criteria like uniformity in order to ensure that the operation is well-founded. For example,
consider 
\begin{gather*}
  f: (0, \infty) \times (0, \infty) \to \R, \\
  f(x,y) = \frac{x}{y}; \\
  \lim_{x\to 0} \left[ \lim_{y\to 0} f(x,y) \right] = \lim_{x\to 0} \infty = \infty, \\
  \lim_{y\to 0} \left[ \lim_{x\to 0} f(x,y) \right] = \lim_{y\to 0} 0 = 0, 
\end{gather*}
so something about this function isn't ``nice'' enough to let us interchange the order of limits.
Consider also
\begin{gather*}
  f: [0,1] \times (0, \infty) \to \R, \\
  f(x,y) = x^y;
\end{gather*}
then $$h(x) = \lim_{y\to 0} g(x,y) = \begin{cases} 0 &x = 0 \\ 1 &\text{ otherwise.} \end{cases}$$

\begin{theorem}
  Suppose $X,Y$ metric spaces, with $X$ connected, and $f: X\to Y$ is continuous. Then $f(X)$ is
  connected.
\end{theorem}
\begin{proof}
  Suppose not; that is, suppose that $f(X)$ is not connected. So write $f(X) = A\cup B$ where $A,B$
  are such that $$\conj{A} \cap B = A \cap \conj{B} = \emptyset,$$ and $A,B$ non-empty. So then $X =
  G \cup H$ where $G = f^{-1}(A), H = f^{-1}(B)$. Obviously, $G,H$ are non-empty; we need to show
  that $\conj{G} \cap H = \emptyset.$ Since $f$ is continuous, $f^{-1}(\conj{A})$ is closed and
  contains $G = f^{-1}(A)$. Thus, $\conj{G} \subset f^{-1}(\conj{A})$. So then $$\conj{G} \cap H
  \subset f^{-1}(\conj{A}) \cap H = f^{-1}(A) \cap f^{-1}(B) \subset f(\conj{A} \cap B) =
  \emptyset.$$ Similarly, we have that $G\cap\conj{H} = \emptyset,$ and so we have that $X$ is not
  connected, which is a contradiction.
\end{proof}

\begin{corollary}[Intermediate Value Theorem]
  For $a<b \in \R$, $f: [a,b] \to \R$, $c\in\R \st f(a) < c < f(b)$, $$\exists x\in [a,b] \st f(x) =
  c.$$
\end{corollary}

\section{Differentiation}
\subsection{The Derivative}

\begin{defn}[Difference Quotient]
  We define the \emph{difference quotient} of a function $f$ at the point $p$ to be
  \begin{gather*}
    \phi: (a,b) \setminus \{p\} \to \R, \\
    x \mapsto \frac{f(x) - f(p)}{x - p}
  \end{gather*}
\end{defn}

\begin{defn}[Differentiability]
  We say that $f$ is \emph{differentiable} at $p$ iff $$\lim_{x\to p} \phi(x)$$ exists. In that case,
  we say that $f'(x) = \lim_{x\to p} \phi(x)$.
\end{defn}

\begin{theorem}
  If $f$ is differentiable at $p$, then $f$ is continuous.
\end{theorem}
\begin{proof}
  Note that $$f(x) = f(p) + \phi(x)(x-p),$$ so 
  \begin{align*}
    lim_{x\to p} f(x) &= \lim_{x\to p} \left[ f(p) + \phi(x)(x-p) \right] \\
                      &= f(p) + \lim_{x\to p} \phi(x)(x-p) \\
                      &= f(p),
  \end{align*}
  since $f$ is differentiable and thus the latter limit exists.
\end{proof}

%lecture 16
Now we have that, if $f$ is differentiable at $x$, then $$f(t) = f(x) + \left[ f'(x) + \varepsilon(t) \right]
(t-x)$$ defines a function $\varepsilon: (a,b) \to \R$ with the property that $$\lim_{t\to x} \varepsilon(t)
= 0.$$ Note that $$t\neq x, \varepsilon(t) = \frac{f(t) - f(x)}{t - x} - f'(x).$$

\begin{theorem}[Combining differentiable functions]
  Suppose $f,g: (a,b) \to \R$ are differentiable at $x \in (a,b)$. Then 
  \begin{enumerate}
    \item $f + g$ is differentiable at $x$ and $(f + g)'(x) = f'(x) + g'(x)$.
    \item $(fg)'(x) = f'(x)g(x) + f(x)g'(x).$
    \item If $g(x) \neq 0$ then $$\left( \frac{f}{g} \right)'(x) = \frac{g(x)f'(x) - f(x)g'(x)}{g(x)^2}.$$
  \end{enumerate}
\end{theorem}
\begin{proof}\leavevmode
  \begin{enumerate}
    \item 
      \begin{align*}
        \lim_{t\to x} \frac{(f+g)(t) - (f+g)(x)}{t -x} &= \lim_{t\to x} \frac{f(t) - f(x)}{t - x} + \lim_{t\to x} \frac{g(t) - g(x)}{t - x} \\
        &= f'(x) + g'(x)
      \end{align*}

    \item Same thing but add and subtract a ``cross term'' $f(x)g(t)$.

    \item Since $g$ is differentiable at $x$, it is continuous at $x$, so 
      $$\exists \delta >0 \st t \in (x-\delta, x+\delta) \Rightarrow g(t) \neq 0.$$ Suppose $t$ is in 
      this interval. Then 
      \begin{align*}
        \frac{\frac{f(t)}{g(t)} - \frac{f(x)}{g(x)}}{t-x} &= \frac{f(t)g(x) - f(x)g(t)}{(t-x)g(t)g(x)} \\
                                        &= \frac{f(t)g(x) - f(x)g(x) - f(x)g(t) + f(x)g(x)}{(t-x)g(t)g(x)} \\
                                        &= \frac{\frac{f(t) - f(x)}{t-x}g(x) - f(x)\frac{g(t)-g(x)}{t-x}}{g(t)g(x)},
      \end{align*}
      and taking all limits we see that we have the result.
  \end{enumerate}
\end{proof}

\begin{theorem}[Linearity]
  Differentiation is a linear operator.  
\end{theorem}
\begin{proof}
  Use (i) above and (ii) with $g = \lambda\in\R$.
\end{proof}

\begin{theorem}[The Chain Rule]
  Suppose we have $f,g$ such that $g \circ f: (a,b) \to \R$ is defined. If $f$ is differentiable at  
  $x$ and $g$ is differentiable at $f(x)$, then $$(g \circ f)'(x) = g'(f(x))f'(x).$$
\end{theorem}
\begin{proof}
  From the definition of differentiability, we have that 
  \begin{align*}
    f(t) &= f(x) + \left[ f'(x) + \varepsilon_1(t)  \right](t-x) \\
    g(t') &= g(x') + \left[ g'(x') + \varepsilon_2(t')  \right](t'-x'),
  \end{align*}
  where $x' = f(x)$ and both error terms vanish.
  Thus we have that 
  \begin{align}
    g(f(t)) &= g \left( f(x) + \left[ f'(x) + \varepsilon_1(t) \right](t-x) \right) \\
            &= g(f(x)) + \left[ g'(f(x)) + \varepsilon_2(t') \right] \left[ f'(x) + \varepsilon_1(t) \right] (t-x),
  \end{align}
  where $$t' = f(x) + \left[ f'(x) + \varepsilon_1(t) \right](t-x).$$ So then forming the difference quotient,
  we have that 
  \begin{align*}
    \frac{g(f(t)) - g(f(x))}{t - x} &= \left[ g'(f(x)) + \varepsilon_2(t') \right] \left[ f'(x) + \varepsilon_1(t) \right].
  \end{align*}
  Now take limits, observing that $\lim_{t\to x}t' = f(x)$. And so 
  $$\lim_{t\to x} \frac{g(f(t)) - g(f(x))}{t - x} = g'(f(x))f'(x),$$ as desired.
\end{proof}

\subsection{Extrema}
\begin{defn}[Local Extrema]
  Suppose $f$ is a real-valued function on a metric space $X$. Then $f$ has a \emph{local minimum} at 
  $x$ if $$\exists \delta > 0 \st \forall t\in N_{\delta}(x), f(t) \geq f(x).$$ The definition of a 
  \emph{local maximum} is analogous.
\end{defn}

\begin{theorem}
  Suppose that $f$ has a local extrema at $x$, and that $f$ is differentiable at $x$. Then $f'(x) = 0$.
\end{theorem}
\begin{proof}
  WLOG suppose that $f$ has a local minimum at $x$. Then choose $$\delta > 0 \st \forall t\in (x-\delta,
  x+\delta), f(t) \geq f(x).$$ Then $$\phi(t) = \frac{f(t) - f(x)}{t - x} \begin{cases} \geq 0 
  &\text{if } $t > x$ \\ \leq 0 &\text{if } $t < x$. \end{cases}$$ Now, taking limits as $t\to x$ from both
    sides, we have that $f'(x) \geq 0$ and $f'(x) \leq 0$, so $f'(x) = 0$. 
\end{proof}

\begin{theorem}[The Mean Value Theorem]
  Let $f: [a,b] \to \R$ be continuous and differentiable on the interval. Then $\exists x\in (a,b)$
  such that $$f'(x) = \frac{f(b) - f(a)}{b-a}.$$
\end{theorem}
\begin{proof}
  BLAH
\end{proof}

%lecture 20
\begin{theorem}[Intermediate Value property of $f'$]
  Let $a<b\in\R$, $f:(a,b) \to \R$ a differentiable function. Suppose $x_1, x_2 \in (a,b)$ and
  $\lambda\in\R$ such that $f'(x_1) < \lambda < f'(x_2).$ Then there exists $x$ between $x_1, x_2$
  Note that this does \emph{not} imply that $f'$ is continuous.
  such that $f'(x) = \lambda.$ 
\end{theorem}

\begin{lemma}
  Suppose $g(x_1) < 0$. Then $\exists \delta_1 > 0$ such that $g(t) < g(x_1)$ for all $t\in (x_1,
  x_1 + \delta_1)$.
\end{lemma}
\begin{proof}
  If not, then in every interval $(x, x+\delta)$, there exists $t$ with $g(t) \geq g(x_1)$. Then 
  $$\frac{g(t) - g(x_1)}{t - x_1} \geq 0,$$ and so the limit has to be $\geq 0$. So then we have
  that $g'(x_1) \geq 0$, which is a contradiction. Thus, there must be some interval $(x,
  x+\delta_1)$ such that the claim holds. \\

  An analogous argument holds for $g(x_1) > 0$.
\end{proof}
\begin{proof}
  Suppose WLOG that $x_1 < x_2$ (and $f'(x_1) < f'(x_2)$). Consider $g(t) = f(t) - \lambda t$
  defined on $(a,b)$. $g$ is differentiable since $f$ is assumed to be differentiable. So we want to
  find $x\in(x_1, x_2)$ such that $g'(x) = f'(x) - \lambda = 0$. If we can find an extrema of $g$ in
  $(a,b)$, then we get that $g'(x) = 0$ for some $x$ for free. Since $[x_1, x_2]$ is compact and $g$
  is continuous (since it is differentiable), it must have a global minimum on the interval. Since
  $g'(x_1) < 0$, we have that there exists $\delta_1 > 0$ such that $g(t) < g(x_1)$ for $t\in (x_1,
  x_1+ \delta_1)$ by the lemma. Similarly, there exists $\delta_2 > 0$ such that for $t\in (x_2 -
  \delta_2, x_2)$, we have that $g(t) < g(x_2)$. Thus, the global minimum does not occur on the
  endpoints, and so is in the interval $(x_1, x_2)$. Thus, we have the claim.
\end{proof}

\begin{theorem}[Extended Mean Value Theorem]
  Suppose we have $f,g: [a,b] \to \R$ continuous and differentiable on the open interval $(a,b)$.
  Then $\exists x\in (a,b)$ such that $$f'(x)[g(b) - g(a)] = g'(x)[f(b) - f(a)].$$ 
\end{theorem}
Perhaps it's more intuitive to say that $$\frac{f'(x)}{g'(x)} = \frac{f(b) - f(a)}{g(b) - g(a)}$$ for
some $x\in (a,b)$. That is, the ratio of the instantaneous changes is equal to the ratio of the
average change (divide top and bottom of the RHS by $(b-a)$) for two functions $f,g$ somewhere in
the interval.
\begin{proof}
  
\end{proof}

\begin{theorem}[L'H\^opital's Rule]
  Suppose that $f,g: (a,b) \to \R$ are differentiable, $g'(t) \neq 0$ for all $t\in(a,b)$,
  $$\lim_{t\to a} f(t) = \lim_{t\to a}g(t) = 0,$$ and $$\lim_{t\to a} \frac{f'(t)}{g'(t)}$$ exists
  and is equal to some value $A$. Then $$\lim_{t\to a} \frac{f(t)}{g(t)} = A.$$ 
\end{theorem}
\begin{proof}
  Fix $\varepsilon > 0$. Then we must find $\delta > 0$ such that for $t\in (a, a + \delta)$, 
  $$\left| \frac{f(t)}{g(t)} - A \right| < \varepsilon.$$
  Choose $c\in (a,b)$ such that for $t\in (a,c)$, we have that 
  $$\frac{f'(t)}{g'(t)} < A + \frac{\varepsilon}{2}.$$
  Now take $a < x < y < c.$ Then, by the extended mean value theorem, we have $t\in (x,y)$ such that 
  $$\frac{f(y) - f(x)}{g(y) - g(x)}  = \frac{f'(t)}{g'(t)} < A + \frac{\varepsilon}{2}.$$
  Now, take $x\to a$. We find that 
  $$\frac{f(y)}{g(y)} \leq A + \frac{\varepsilon}{2} < A + \varepsilon.$$ 
  And so for $y\in(a,c)$, we have that 
  $$\frac{f(y)}{g(y)} - A < \varepsilon.$$ We repeat the argument for the other side of the
  inequality and the intersect the interval to satisfy the claim.
\end{proof}

Another proof.
\begin{proof}
  Note that, by the intermediate value property of derivatives, we have that since $g'(x) \neq 0$,
  either $g' > 0$ or $g' < 0$. So, $g$ is strictly increasing or decreasing, respectively. Fix
  $\varepsilon > 0$; we must find $\delta > 0$ such that, for all $t\in (a, a + \delta)$, we have
  that $$\left| \frac{f(t)}{g(t)} - A \right| < \varepsilon.$$ The rest of the proof follows as
  above (edit these together).
\end{proof}

\begin{theorem}[Taylor's Theorem]
  Suppose that $f: (a,b) \to \R$ is $n$ times differentiable for $n\in\N$. Fix $\alpha, \beta\in
  (a,b)$ and suppose $\alpha\neq\beta$. Then let
  \begin{align*}
    P(t) &= \sum_{k=0}^{n-1} \frac{f^{(k)}(\alpha)}{k!}(t-\alpha)^k \\
         &= f(\alpha) + f'(\alpha)(t-\alpha) + \frac{f''(\alpha)}{2!}(t-\alpha)^2 + \dots +
    \frac{f^{(n-1)}(\alpha)}{(n-1)!}(t-\alpha)^{n-1}.
  \end{align*}
  Then $\exists x$ between $\alpha, \beta$ such that $$f(\beta) = P(\beta) +
  \frac{f^{(n)}(x)}{n!}(\beta-\alpha)^n,$$ where the second term is the error. 
\end{theorem}
\begin{proof}
  Define $M$ such that $f(\beta) - P(\beta) - M(\beta - \alpha)^n = 0$, and define $g:(a,b)\to\R$ as
  $$g(t) = f(t) - P(t) - M(t-\alpha)^n.$$ Note that since $f$ is $n$-times differentiable and the
  other terms are polynomials, we have that $g$ is $n$-times differentiable. So then note that
  $g(\alpha) = 0$, and $g'(\alpha) = 0$, and so on until $g^{(n-1)}(\alpha) = 0$. Further, by the
  definition of $M$, we have that $g(\beta) = 0$. Finally, note that 
  $$g^{(n)}(t) = f^{(n)}(t) - n!M.$$ Note that we have the result if $g^{(n)} = 0$, so then we seek
  $x$ between $\alpha, \beta$ such that $g^{(n)} = 0.$ Since we have that $g$ is $n$-times
  differentiable, and so $g, \ldots, g^{(n-1)}$ are continuous, and that each vanishes at $\alpha$,
  we can repeatedly apply the MVT to find an $x$ such that $g^{(n)} = 0$ (formally, this is via
  induction).
\end{proof}

\section{Integration}

\end{document}
